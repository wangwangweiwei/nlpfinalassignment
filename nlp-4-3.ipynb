{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3645a21-e8c5-403d-996f-a6af6a84c781",
    "_uuid": "a0c0e6d8-4159-4f43-9d47-217663aa79f1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-02-06T12:06:20.099400Z",
     "iopub.status.busy": "2023-02-06T12:06:20.099025Z",
     "iopub.status.idle": "2023-02-06T12:06:21.492390Z",
     "shell.execute_reply": "2023-02-06T12:06:21.491424Z",
     "shell.execute_reply.started": "2023-02-06T12:06:20.099370Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data_file = '/kaggle/working/sbert_text_similarity/data/atec_nlp_sim_train_all.csv'\n",
    "    data = pd.read_csv(data_file, sep='\\t', header=None, names=['index', 's1', 's2', 'label'])\n",
    "    # 获取数据和标签\n",
    "    x = data[['s1', 's2']].values.tolist()\n",
    "    y = data['label'].values.tolist()\n",
    "    # 划分训练集和测试集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123, shuffle=True)\n",
    "    print(x_train[0], y_train[0])\n",
    "    print('总共有数据：{}条，其中正样本：{}条，负样本：{}条'.format(\n",
    "        len(x), sum(y), len(x) - sum(y)))\n",
    "    print('训练数据：{}条,其中正样本：{}条，负样本：{}条'.format(\n",
    "        len(x_train), sum(y_train), len(x_train) - sum(y_train)))\n",
    "    print('测试数据：{}条,其中正样本：{}条，负样本：{}条'.format(\n",
    "        len(x_test), sum(y_test), len(x_test) - sum(y_test)))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a8ba3920-0851-4a19-920c-82778190b137",
    "_uuid": "2cc05231-2f65-4264-aea4-b72475c04522",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-02-06T12:06:51.369208Z",
     "iopub.status.busy": "2023-02-06T12:06:51.368848Z",
     "iopub.status.idle": "2023-02-06T12:07:04.580774Z",
     "shell.execute_reply": "2023-02-06T12:07:04.579406Z",
     "shell.execute_reply.started": "2023-02-06T12:06:51.369179Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "64493ea2-6685-4322-a230-121bef8295ab",
    "_uuid": "8069a28e-2629-4ecd-b267-288eb98e222b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-02-06T12:07:40.810554Z",
     "iopub.status.busy": "2023-02-06T12:07:40.810157Z",
     "iopub.status.idle": "2023-02-06T12:17:56.219153Z",
     "shell.execute_reply": "2023-02-06T12:17:56.217900Z",
     "shell.execute_reply.started": "2023-02-06T12:07:40.810519Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import  SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers import models, evaluation\n",
    "#from preprocess import get_data\n",
    "\n",
    "model_path = '/kaggle/input/hflchineserobertawwmext/'\n",
    "word_embedding_model = models.Transformer(\"hfl/chinese-roberta-wwm-ext\", max_seq_length=64)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(),\n",
    "                           out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_data()\n",
    "train_examples = []\n",
    "for s, label in zip(x_train, y_train):\n",
    "    s1, s2 = s\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[s1, s2], label=float(label))\n",
    "    )\n",
    "test_examples = []\n",
    "for s, label in zip(x_test, y_test):\n",
    "    s1, s2 = s\n",
    "    test_examples.append(\n",
    "        InputExample(texts=[s1, s2], label=float(label))\n",
    "    )\n",
    "train_loader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model_save_path = '/kaggle/working/'\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(test_examples)\n",
    "model.fit(train_objectives=[(train_loader, train_loss)],\n",
    "          epochs=1,\n",
    "          evaluator=evaluator,\n",
    "          warmup_steps=100,\n",
    "          save_best_model=True,\n",
    "          output_path=model_save_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95068fa7-0df8-4793-8bb7-631d6cb27941",
    "_uuid": "d73490e3-d75a-4d03-9dfd-2f50894940e9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-02-06T12:17:56.226256Z",
     "iopub.status.busy": "2023-02-06T12:17:56.225075Z",
     "iopub.status.idle": "2023-02-06T12:18:36.937389Z",
     "shell.execute_reply": "2023-02-06T12:18:36.936292Z",
     "shell.execute_reply.started": "2023-02-06T12:17:56.226207Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#from preprocess import get_data\n",
    "\n",
    "model_path = '/kaggle/working/'\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = get_data()\n",
    "s1 = np.array(x_test)[:, 0]\n",
    "s2 = np.array(x_test)[:, 1]\n",
    "embedding1 = model.encode(s1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(s2, convert_to_tensor=True)\n",
    "pre_labels = [0] * len(s1)\n",
    "predict_file = open('predict.txt', 'w')\n",
    "for i in range(len(s1)):\n",
    "    similarity = util.cos_sim(embedding1[i], embedding2[i])\n",
    "    if similarity > 0.5:\n",
    "        pre_labels[i] = 1\n",
    "    predict_file.write(s1[i] + ' ' +\n",
    "                       s2[i] + ' ' +\n",
    "                       str(y_test[i]) + ' ' +\n",
    "                       str(pre_labels[i]) + '\\n')\n",
    "print(classification_report(y_test, pre_labels))\n",
    "predict_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
