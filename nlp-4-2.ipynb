{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T13:02:04.031921Z",
     "iopub.status.busy": "2023-02-06T13:02:04.031117Z",
     "iopub.status.idle": "2023-02-06T13:02:07.878486Z",
     "shell.execute_reply": "2023-02-06T13:02:07.877077Z",
     "shell.execute_reply.started": "2023-02-06T13:02:04.031803Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from transformers import BertTokenizerFast\n",
    "#from transformers import logging\n",
    "\n",
    "# 设置transformers模块的日志等级，减少不必要的警告，对训练过程无影响，请忽略\n",
    "#logging.set_verbosity_error()\n",
    "\n",
    "# 环境变量：设置程序能使用的GPU序号。例如：\n",
    "# 当前服务器有8张GPU可用，想用其中的第2、5、8卡，这里应该设置为:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,4,7\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T13:02:07.888108Z",
     "iopub.status.busy": "2023-02-06T13:02:07.885180Z",
     "iopub.status.idle": "2023-02-06T13:02:07.902523Z",
     "shell.execute_reply": "2023-02-06T13:02:07.901410Z",
     "shell.execute_reply.started": "2023-02-06T13:02:07.888068Z"
    }
   },
   "outputs": [],
   "source": [
    "# 通过继承nn.Module类自定义符合自己需求的模型\n",
    "class BertNERModel(nn.Module):\n",
    "    # 初始化类\n",
    "    def __init__(self, ner_labels, pretrained_name='bert-base-chinese'):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            ner_labels  :指定分类模型的最终类别数目，以确定线性分类器的映射维度\n",
    "            pretrained_name :用以指定bert的预训练模型\n",
    "        \"\"\"\n",
    "        super(BertNERModel, self).__init__()\n",
    "        # 加载HuggingFace的BertModel\n",
    "        # BertModel的最终输出维度默认为768\n",
    "        # return_dict=True 可以使BertModel的输出具有dict属性，即以 bert_output['last_hidden_state'] 方式调用\n",
    "        self.bert = BertModel.from_pretrained(pretrained_name,\n",
    "                                              return_dict=True)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        # 通过一个线性层将标签对应的维度：768->class_size\n",
    "        self.classifier = nn.Linear(768, ner_labels)\n",
    "    def forward(self, inputs):\n",
    "        # 获取DataLoader中已经处理好的输入数据：\n",
    "        # input_ids :tensor类型，shape=batch_size*max_len   max_len为当前batch中的最大句长\n",
    "        # input_tyi :tensor类型，\n",
    "        # input_attn_mask :tensor类型，因为input_ids中存在大量[Pad]填充，attention mask将pad部分值置为0，让模型只关注非pad部分\n",
    "        input_ids, input_tyi, input_attn_mask = inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask']\n",
    "        output = self.bert(input_ids, input_tyi, input_attn_mask)\n",
    "        # bert_output 分为两个部分：\n",
    "        #   last_hidden_state:最后一个隐层的值\n",
    "        #   pooler output:对应的是[CLS]的输出,用于分类任务\n",
    "        # categories_numberic：tensor类型，shape=batch_size*class_size，用于后续的CrossEntropy计算\n",
    "        categories_numberic = self.classifier(output.last_hidden_state)\n",
    "        batch_size, seq_len, ner_class_num = categories_numberic.shape\n",
    "        categories_numberic = categories_numberic.view((batch_size * seq_len, ner_class_num))\n",
    "        return categories_numberic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-11T08:22:40.313656Z",
     "iopub.status.busy": "2023-02-11T08:22:40.313287Z",
     "iopub.status.idle": "2023-02-11T08:22:40.451181Z",
     "shell.execute_reply": "2023-02-11T08:22:40.450245Z",
     "shell.execute_reply.started": "2023-02-11T08:22:40.313621Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def dataToJsionl(path):\n",
    "    dataset_dict = {}\n",
    "    count = 0\n",
    "    with open(path, 'r', encoding = \"utf8\") as f:\n",
    "        while True:\n",
    "            #f.readline().split(\"\\n\")[0]内存要爆炸，可能是1变2\n",
    "            sents = f.readline().replace('\\n','')\n",
    "            if not sents:\n",
    "                break\n",
    "            ner_labels = f.readline().replace('\\n','')\n",
    "            dataset_dict[count] = {\"sents\": ''.join(sents), \"ner_labels\": ''.join(ner_labels)}\n",
    "            count += 1\n",
    "    with open(path.split('/')[-1].split('.')[0]+'.jsonl','w', encoding = \"utf8\") as file:\n",
    "        file.write(json.dumps(dataset_dict,indent = 4))\n",
    "dataToJsionl(\"/kaggle/input/youkunerdataset/train.txt\")\n",
    "dataToJsionl(\"/kaggle/input/youkunerdataset/dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-06T13:02:16.269280Z",
     "iopub.status.busy": "2023-02-06T13:02:16.268574Z",
     "iopub.status.idle": "2023-02-06T13:06:54.596242Z",
     "shell.execute_reply": "2023-02-06T13:06:54.595204Z",
     "shell.execute_reply.started": "2023-02-06T13:02:16.269215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea18722723545a5a896d04b62491b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6688ab9a694711accfdc66bddc8d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/393M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabd934c2fc4489580d18b3a56585593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606611ceb16a438881bf933d4f6c1383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2ba003ecc345119effc03835c175c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 251/251 [00:42<00:00,  5.97it/s]\n",
      "Deving: 100%|██████████| 1000/1000 [00:07<00:00, 127.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.8122923588039868\n",
      "recall is 0.6485411140583555\n",
      "f1 is 0.7212389380530974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 251/251 [00:41<00:00,  6.11it/s]\n",
      "Deving: 100%|██████████| 1000/1000 [00:08<00:00, 120.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.818521284540702\n",
      "recall is 0.726790450928382\n",
      "f1 is 0.769933263083948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 251/251 [00:41<00:00,  6.07it/s]\n",
      "Deving: 100%|██████████| 1000/1000 [00:08<00:00, 123.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.8243055555555555\n",
      "recall is 0.7871352785145889\n",
      "f1 is 0.8052917232021709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 251/251 [00:41<00:00,  6.09it/s]\n",
      "Deving: 100%|██████████| 1000/1000 [00:08<00:00, 124.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.7879353233830846\n",
      "recall is 0.8401856763925729\n",
      "f1 is 0.813222079589217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 251/251 [00:41<00:00,  6.11it/s]\n",
      "Deving: 100%|██████████| 1000/1000 [00:08<00:00, 120.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.788\n",
      "recall is 0.7838196286472149\n",
      "f1 is 0.7859042553191491\n"
     ]
    }
   ],
   "source": [
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))\n",
    "\n",
    "\n",
    "def load_sentence_nertags(data_path):\n",
    "    all_data = []\n",
    "    with open(data_path, 'r', encoding=\"utf8\") as file:\n",
    "        res_dict = json.load(file)\n",
    "    for id, item in res_dict.items():\n",
    "        sent = item['sents'].split(' ')\n",
    "        ner_labels = item['ner_labels'].split(' ')\n",
    "        all_data.append((sent, ner_labels))\n",
    "    return all_data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "torch提供了优秀的数据加载类Dataloader，可以自动加载数据。\n",
    "1. 想要使用torch的DataLoader作为训练数据的自动加载模块，就必须使用torch提供的Dataset类\n",
    "2. 一定要具有__len__和__getitem__的方法，不然DataLoader不知道如何如何加载数据\n",
    "这里是固定写法，是官方要求，不懂可以不做深究，一般的任务这里都通用\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data_size = len(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 这里可以自行定义，Dataloader会使用__getitem__(self, index)获取数据\n",
    "        # 这里我设置 self.dataset[index] 规定了数据是按序号取得，序号是多少DataLoader自己算，用户不用操心\n",
    "        return self.dataset[index]\n",
    "\n",
    "\n",
    "def coffate_fn(examples):\n",
    "    sents, all_labels = [], []\n",
    "    for sent, ner_labels in examples:\n",
    "        sents.append(sent)\n",
    "        all_labels.append([categories.get(label, 0) for label in ner_labels])\n",
    "    tokenized_inputs = tokenizer(sents,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_offsets_mapping=True,\n",
    "                                 is_split_into_words=True,\n",
    "                                 max_length=512,\n",
    "                                 return_tensors=\"pt\")\n",
    "    targets = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        label_ids = []\n",
    "        for word_idx in tokenized_inputs.word_ids(batch_index=i):\n",
    "            # 将特殊符号的标签设置为-100，以便在计算损失函数时自动忽略\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                # 把标签设置到每个词的第一个token上\n",
    "                label_ids.append(labels[word_idx])\n",
    "        targets.append(label_ids)\n",
    "    targets = torch.tensor(targets)\n",
    "    return tokenized_inputs, targets\n",
    "\n",
    "\n",
    "def split_entity(label_sequence):\n",
    "    entity_mark = dict()\n",
    "    entity_pointer = None\n",
    "    for index, label in enumerate(label_sequence):\n",
    "        if label.startswith('B'):\n",
    "            category = label.split('-')[1]\n",
    "            entity_pointer = (index, category)\n",
    "            entity_mark.setdefault(entity_pointer, [label])\n",
    "        elif label.startswith('I'):\n",
    "            if entity_pointer is None:\n",
    "                continue\n",
    "            if entity_pointer[1] != label.split('-')[1]:\n",
    "                continue\n",
    "            entity_mark[entity_pointer].append(label)\n",
    "        else:\n",
    "            entity_pointer = None\n",
    "    return entity_mark\n",
    "\n",
    "\n",
    "def evaluate(real_label, predict_label):\n",
    "    # 序列标注的准确率和召回率计算，详情查看：https://zhuanlan.zhihu.com/p/56582082\n",
    "    real_entity_mark = split_entity(real_label)\n",
    "    predict_entity_mark = split_entity(predict_label)\n",
    "\n",
    "    true_entity_mark = dict()\n",
    "    key_set = real_entity_mark.keys() & predict_entity_mark.keys()\n",
    "    for key in key_set:\n",
    "        real_entity = real_entity_mark.get(key)\n",
    "        predict_entity = predict_entity_mark.get(key)\n",
    "        if tuple(real_entity) == tuple(predict_entity):\n",
    "            true_entity_mark.setdefault(key, real_entity)\n",
    "\n",
    "    real_entity_num = len(real_entity_mark)\n",
    "    predict_entity_num = len(predict_entity_mark)\n",
    "    true_entity_num = len(true_entity_mark)\n",
    "\n",
    "    precision = true_entity_num / predict_entity_num\n",
    "    recall = true_entity_num / real_entity_num\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# 训练准备阶段，设置超参数和全局变量\n",
    "\n",
    "batch_size = 32\n",
    "num_epoch = 5  # 训练轮次\n",
    "check_step = 5  # 用以训练中途对模型进行检验：每check_step个epoch进行一次测试和保存模型\n",
    "train_data_path = \"/kaggle/working/train.jsonl\"  # 数据所在地址\n",
    "dev_data_path = \"/kaggle/working/dev.jsonl\"\n",
    "learning_rate = 1e-5  # 优化器的学习率\n",
    "\n",
    "# 获取训练、测试数据、分类类别总数\n",
    "train_data = load_sentence_nertags(data_path =  train_data_path)\n",
    "dev_data = load_sentence_nertags(data_path =  dev_data_path)\n",
    "\n",
    "categories = {\n",
    "    'O': 0,\n",
    "    'B-MISC': 1,\n",
    "    'I-MISC': 2,\n",
    "    'E-MISC': 3,\n",
    "    'S-MISC': 4,\n",
    "    'B-TELEVISION': 5,\n",
    "    'I-TELEVISION': 6,\n",
    "    'E-TELEVISION': 7,\n",
    "    'S-TELEVISION': 8,\n",
    "    'B-PER': 9,\n",
    "    'I-PER': 10,\n",
    "    'E-PER': 11,\n",
    "    'S-PER': 12,\n",
    "    0: 'O', \n",
    "    1: 'B-MISC', \n",
    "    2: 'I-MISC', \n",
    "    3: 'E-MISC', \n",
    "    4: 'S-MISC', \n",
    "    5: 'B-TELEVISION', \n",
    "    6: 'I-TELEVISION', \n",
    "    7: 'E-TELEVISION', \n",
    "    8: 'S-TELEVISION', \n",
    "    9: 'B-PER', \n",
    "    10: 'I-PER', \n",
    "    11: 'E-PER', \n",
    "    12: 'S-PER'\n",
    "}\n",
    "\n",
    "# 将训练数据和测试数据的列表封装成Dataset以供DataLoader加载\n",
    "train_dataset = BertDataset(train_data)\n",
    "dev_dataset = BertDataset(dev_data)\n",
    "\"\"\"\n",
    "DataLoader主要有以下几个参数：\n",
    "Args:\n",
    "    dataset (Dataset): dataset from which to load the data.\n",
    "    batch_size (int, optional): how many samples per batch to load(default: ``1``).\n",
    "    shuffle (bool, optional): set to ``True`` to have the data reshuffled at every epoch (default: ``False``).\n",
    "    collate_fn : 传入一个处理数据的回调函数\n",
    "DataLoader工作流程：\n",
    "1. 先从dataset中取出batch_size个数据\n",
    "2. 对每个batch，执行collate_fn传入的函数以改变成为适合模型的输入\n",
    "3. 下个epoch取数据前先对当前的数据集进行shuffle，以防模型学会数据的顺序而导致过拟合\n",
    "\"\"\"\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              collate_fn=coffate_fn,\n",
    "                              shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                             batch_size=1,\n",
    "                             collate_fn=coffate_fn,\n",
    "                             shuffle=True)\n",
    "\n",
    "#固定写法，可以牢记，cuda代表Gpu\n",
    "# torch.cuda.is_available()可以查看当前Gpu是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练模型，因为这里是英文数据集，需要用在英文上的预训练模型：bert-base-uncased\n",
    "# uncased指该预训练模型对应的词表不区分字母的大小写\n",
    "# 详情可了解：https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'bert-base-chinese'\n",
    "\n",
    "model = BertNERModel(len(categories) // 2, pretrained_model_name)\n",
    "# 固定写法，将模型加载到device上，\n",
    "# 如果是GPU上运行，此时可以观察到GPU的显存增加\n",
    "model.to(device)\n",
    "# 加载预训练模型对应的tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# 训练过程\n",
    "# Adam是最近较为常用的优化器，详情可查看：https://www.jianshu.com/p/aebcaf8af76e\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "            0.01\n",
    "    }, {\n",
    "        'params':\n",
    "            [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay':\n",
    "            0.0\n",
    "}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, learning_rate, eps = 1e-8)  # 使用Adam优化器\n",
    "CE_loss = nn.CrossEntropyLoss(ignore_index=-100)  # 使用crossentropy作为分类任务的损失函数\n",
    "\n",
    "# 记录当前训练时间，用以记录日志和存储\n",
    "timestamp = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "# 开始训练，model.train()固定写法，详情可以百度\n",
    "model.train()\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    # 记录当前epoch的总loss\n",
    "    total_loss = 0\n",
    "    # tqdm用以观察训练进度，在console中会打印出进度条\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\"):\n",
    "        # tqdm(train_dataloader, desc=f\"Training Epoch {epoch}\") 会自动执行DataLoader的工作流程，\n",
    "        # 想要知道内部如何工作可以在debug时将断点打在 coffate_fn 函数内部，查看数据的处理过程\n",
    "\n",
    "        # 对batch中的每条tensor类型数据，都执行.to(device)，\n",
    "        # 因为模型和数据要在同一个设备上才能运行\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        targets = targets.view(-1)\n",
    "        # 清除现有的梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 模型前向传播，model(inputs)等同于model.forward(inputs)\n",
    "        bert_output = model(inputs)\n",
    "\n",
    "        # 计算损失，交叉熵损失计算可参考：https://zhuanlan.zhihu.com/p/159477597\n",
    "        loss = CE_loss(bert_output, targets)\n",
    "\n",
    "        # 梯度反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 根据反向传播的值更新模型的参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计总的损失，.item()方法用于取出tensor中的值\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    #测试过程\n",
    "    target_labels = []\n",
    "    pred_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_dataloader, desc=f\"Deving\"):\n",
    "            inputs, targets = [x.to(device) for x in batch]\n",
    "            targets = targets.view(-1)\n",
    "            bert_output = model(inputs)\n",
    "            predictions = bert_output.argmax(dim=-1)\n",
    "            target_labels += [categories[i] for i in targets.tolist() if i != -100]\n",
    "            pred_labels += [categories[i] for i in predictions.tolist()[1:-1] if i != -100]\n",
    "\n",
    "    precision, recall, f1 = evaluate(real_label=target_labels,\n",
    "                                     predict_label=pred_labels)\n",
    "    print(\"precision is {}\\nrecall is {}\\nf1 is {}\".format(\n",
    "        precision, recall, f1))\n",
    "\n",
    "    if epoch % check_step == 0:\n",
    "        # 保存模型\n",
    "        checkpoints_dirname = \"bert_ner_\" + timestamp\n",
    "        os.makedirs(checkpoints_dirname, exist_ok=True)\n",
    "        save_pretrained(model,\n",
    "                        checkpoints_dirname + '/checkpoints-{}/'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T13:10:01.887656Z",
     "iopub.status.busy": "2023-02-06T13:10:01.887284Z",
     "iopub.status.idle": "2023-02-06T13:10:09.467556Z",
     "shell.execute_reply": "2023-02-06T13:10:09.466588Z",
     "shell.execute_reply.started": "2023-02-06T13:10:01.887623Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1001/1001 [00:07<00:00, 139.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.7716635041113219\n",
      "recall is 0.806345009914078\n",
      "f1 is 0.788623141564318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型类必须在别的地方定义\n",
    "dataToJsionl(\"/kaggle/input/youkunerdataset/test.txt\")\n",
    "test_data = load_sentence_nertags(\"/kaggle/working/test.jsonl\")\n",
    "tset_dataset = BertDataset(test_data)\n",
    "test_dataloader = DataLoader(tset_dataset,\n",
    "                             batch_size=1,\n",
    "                             collate_fn=coffate_fn,\n",
    "                             shuffle=True)\n",
    "#model = torch.load(\"/kaggle/working/bert_ner_01_04_11_16/checkpoints-10/model.pth\")\n",
    "model = torch.load(\"/kaggle/working/bert_ner_02_06_13_02/checkpoints-5/model.pth\")\n",
    "model.eval()\n",
    "#测试过程\n",
    "target_labels = []\n",
    "pred_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=f\"Testing\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        targets = targets.view(-1)\n",
    "        bert_output = model(inputs)\n",
    "        predictions = bert_output.argmax(dim=-1)\n",
    "        target_labels += [categories[i] for i in targets.tolist() if i != -100]\n",
    "        pred_labels += [categories[i] for i in predictions.tolist()[1:-1] if i != -100]\n",
    "\n",
    "    precision, recall, f1 = evaluate(real_label=target_labels,\n",
    "                                     predict_label=pred_labels)\n",
    "    print(\"precision is {}\\nrecall is {}\\nf1 is {}\".format(\n",
    "        precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
