{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成实验数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def count_text(category_path):\n",
    "    \"\"\"\n",
    "    统计总体语料的分布情况\n",
    "    :param category_path: 语料路径\n",
    "    :return: 不同种类的语料字典\n",
    "    \"\"\"\n",
    "    if os.path.exists(category_path):\n",
    "        # 语料的路径\n",
    "        category_path = category_path + '/*'  # 匹配所有的目录\n",
    "        total = {}  # 语料总数\n",
    "        for dir in glob(category_path):\n",
    "            total[dir] = len(glob(dir + '/*.txt'))  # 每个类别下文件的数量\n",
    "        print(total)\n",
    "        print(\"文件共{}类,总的文件的数量:{}\".format(len(total), sum(total.values())))\n",
    "    else:\n",
    "        raise FileExistsError('{}文件的路径不存在'.format(category_path))\n",
    "    return total\n",
    "\n",
    "\n",
    "def cut_corpus(path):\n",
    "    \"\"\"\n",
    "    切分语料集 训练集 验证集 测试集，比例0.7:0.15:0.15\n",
    "    :param path: 语料路径list\n",
    "    :return: 切分后的数据集和标签\n",
    "    \"\"\"\n",
    "    label = re.findall(r\"[\\u4e00-\\u9fa5]+\", path)  # 匹配汉字\n",
    "    #label = path.split(\"/\")[-1]\n",
    "    files = glob(path + '/*.txt')  # 匹配txt文件的绝对路径\n",
    "    # 切分数据集\n",
    "    train, test = train_test_split(files, test_size=0.3, shuffle=True, random_state=2020)\n",
    "    valid, test = train_test_split(test, test_size=0.5, shuffle=True, random_state=2021)\n",
    "    print(\"train:{} test:{} valid:{}\".format(len(train), len(test), len(valid)))\n",
    "    return train, test, valid, label\n",
    "\n",
    "\n",
    "def read_data(path, label=None, debug=False, frac=1):\n",
    "    \"\"\"\n",
    "    读取文件中的数据title content\n",
    "    :param path: 每条语料的路径信息list\n",
    "    :param debug: 采样模式\n",
    "    :param frac: 采样的比例\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    contents = []\n",
    "    for file in tqdm(path):\n",
    "        with open(file, 'r', encoding='utf-8') as obj:\n",
    "            data = obj.readlines()\n",
    "        title = data[0].strip()\n",
    "        content = [sen.strip() for sen in data[1:]]\n",
    "        titles.append(title)\n",
    "        contents.append(''.join(content))\n",
    "\n",
    "    title_content = defaultdict(list)\n",
    "\n",
    "    if len(titles) == len(contents):\n",
    "        title_content['title'] = titles\n",
    "        title_content['content'] = contents\n",
    "        title_content['label'] = [label] * len(titles)\n",
    "    else:\n",
    "        raise ValueError('数据titles和contents数量不一致')\n",
    "    df = pd.DataFrame(title_content, columns=['title', 'content', 'label'])\n",
    "    if debug:\n",
    "        # 采样\n",
    "        df = df.sample(frac=frac, random_state=2020).reset_index(drop=True)\n",
    "        print('采样的样本数量{}'.format(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "\n",
    "def writ_to_csv(dictionary, filename='train'):\n",
    "    \"\"\"\n",
    "    将数据写入csv文件\n",
    "    :param dictionary: 字典格式\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(dictionary, columns=['title', 'content', 'label'])\n",
    "    df.to_csv('{}.csv'.format(filename), sep='\\t', index=False)\n",
    "    print()\n",
    "    print('writing succesfully')\n",
    "\n",
    "\n",
    "def process(path, filename='train', frac=1):\n",
    "    \"\"\"\n",
    "    读取数据文件将数据写入csv文件 title content label\n",
    "    :param path: 数据文件的路径dict\n",
    "    :param filename: 保存文件命名\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('loading {}'.format(filename))\n",
    "    sample = []\n",
    "    for label, data in path.items():\n",
    "        under_sample = read_data(data, label, debug=True, frac=frac)\n",
    "        sample.append(under_sample)\n",
    "    df = pd.concat(sample, axis=0)\n",
    "    print(\"{}文件的数据量为:{}\".format(filename, df.shape[0]))\n",
    "    # 保存文件的路径\n",
    "    base_path = \"/kaggle/working\"\n",
    "    save_path = base_path + '/' + filename + '.csv'\n",
    "    df.to_csv(save_path, sep='\\t', index=False)\n",
    "    print('{} writing succesfully'.format(save_path))\n",
    "\n",
    "\n",
    "def write_label_id(train_path,label_path):\n",
    "    \"\"\"标签映射为id\"\"\"\n",
    "    data = pd.read_csv(train_path,header=0, delimiter=\"\\t\").dropna()\n",
    "    label = data['label'].unique()\n",
    "    print('标签:{}'.format(label))\n",
    "    label2id = dict(zip(label, range(len(label))))\n",
    "    json.dump(label2id, open(label_path, 'w', encoding='utf-8'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root_path = \"/kaggle/working\"\n",
    "    category_path = \"/kaggle/input/thucnews/THUCNews/THUCNews\"\n",
    "    # 语料的路径\n",
    "    dir_dict = count_text(category_path)\n",
    "    train_path = defaultdict(list)\n",
    "    test_path = defaultdict(list)\n",
    "    valid_path = defaultdict(list)\n",
    "    for path in dir_dict.keys():\n",
    "        # 切分数据集\n",
    "        train, test, valid, label = cut_corpus(path)\n",
    "        # 保存数据到字典\n",
    "        train_path[label[0]] = train\n",
    "        test_path[label[0]] = test\n",
    "        valid_path[label[0]] = valid\n",
    "\n",
    "    process(train_path, filename='train', frac=0.6)\n",
    "    process(test_path, filename='test', frac=0.5)\n",
    "    process(valid_path, filename='valid', frac=0.5)\n",
    "\n",
    "\n",
    "    train_path = \"/kaggle/working/train.csv\"\n",
    "    label_path = \"/kaggle/working/label2id.json\"\n",
    "    # 生成标签到id的json文件\n",
    "    write_label_id(train_path, label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己调用hugging face库实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rjieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T08:02:46.245976Z",
     "iopub.status.busy": "2023-02-06T08:02:46.245169Z",
     "iopub.status.idle": "2023-02-06T08:02:54.170864Z",
     "shell.execute_reply": "2023-02-06T08:02:54.166344Z",
     "shell.execute_reply.started": "2023-02-06T08:02:46.245873Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM as WoBertForMaskedLM\n",
    "from transformers import RoFormerTokenizer as WoBertTokenizer\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#考虑tokenizer放在collate_fn中还是放在dataset中啦，目前参考以前的例子放在前面\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.data = pd.read_csv(path, header = 0, delimiter = \"\\t\").dropna()\n",
    "        #去除content列\n",
    "        self.data.drop('content', axis = 1, inplace = True)\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        title = self.data.loc[i]['title']\n",
    "        label = self.data.loc[i]['label']\n",
    "        \"\"\"\n",
    "        data = self.data.iloc[i]\n",
    "        return data['title'], data['label']\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T08:03:08.216102Z",
     "iopub.status.busy": "2023-02-06T08:03:08.215452Z",
     "iopub.status.idle": "2023-02-06T08:04:00.735743Z",
     "shell.execute_reply": "2023-02-06T08:04:00.734848Z",
     "shell.execute_reply.started": "2023-02-06T08:03:08.216059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd941fce13f47568bd9ba38d5221f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12007152ff18492298ad70b1d667ac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae6f79410294209b466f899809d9ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c18d26b938144319db8a0a3d9ebc8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/755 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc4379ada4641cb9e8dc92ac877a797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at junnyu/wobert_chinese_base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "path = \"junnyu/wobert_chinese_base\"\n",
    "tokenizer = WoBertTokenizer.from_pretrained(path)\n",
    "model = WoBertForMaskedLM.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/input/download/Downloads/label2id.json', 'r', encoding='utf-8') as f:\n",
    "    label2id = json.load(f)\n",
    "def collate_fn(data):\n",
    "    titles = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    labels = [label2id.get(i) for i in labels]\n",
    "    #编码\n",
    "    data = tokenizer(titles,\n",
    "                    truncation = True,\n",
    "                    padding = True,\n",
    "                    max_length = 128,\n",
    "                    return_tensors = 'pt',\n",
    "                    )\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置为0，其他位置是1\n",
    "    input_ids = data['input_ids'].to(config.device)\n",
    "    attention_mask = data['attention_mask'].to(config.device)\n",
    "    token_type_ids = data['token_type_ids'].to(config.device)\n",
    "    labels = torch.LongTensor(labels).to(config.device)\n",
    "    \n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        model_config = BertConfig.from_pretrained('junnyu/wobert_chinese_base',num_labels = config.num_classes)\n",
    "        self.bert = BertModel.from_pretrained(\"junnyu/wobert_chinese_base\", config = model_config)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[1]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        token_type_ids = x[2]\n",
    "        _, pooled = self.bert(context,\n",
    "                              attention_mask=mask,\n",
    "                              token_type_ids=token_type_ids\n",
    "                             ,return_dict=False)\n",
    "        out = self.fc(pooled)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (AdamW, BertTokenizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(config, model, train_iter, dev_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    print('User AdamW...')\n",
    "    print(config.device)\n",
    "    #初始化参数\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "            0.01\n",
    "    }, {\n",
    "        'params':\n",
    "            [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay':\n",
    "            0.0\n",
    "    }]\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=config.learning_rate,\n",
    "                      eps=config.eps)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, (trains, mask, tokens, labels) in tqdm(enumerate(train_iter)):\n",
    "            trains = trains.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "            mask = mask.to(config.device)\n",
    "            tokens = tokens.to(config.device)\n",
    "            outputs = model((trains, mask, tokens))\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #             scheduler.step()\n",
    "            if total_batch % 1000 == 0 and total_batch != 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                import torch\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                \n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                \n",
    "                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = round(time.time() - start_time, 4)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                \n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "    \n",
    "    #我认为训练的时候不能用训练集测试\n",
    "    #test(config, model, test_iter)\n",
    "    #显存资源比较紧缺，可以在每个epoch开始时释放下不用的显存资源\n",
    "    import torch, gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(config,\n",
    "                                                                model,\n",
    "                                                                test_iter,\n",
    "                                                                test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)\n",
    "    time_dif = round(time.time() - start_time, 4)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "\n",
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, mask, tokens, labels in tqdm(data_iter):\n",
    "            texts = texts.to(config.device)\n",
    "            labels = labels.to(config.device)\n",
    "            mask = mask.to(config.device)\n",
    "            tokens = tokens.to(config.device)\n",
    "            \n",
    "            outputs = model((texts, mask, tokens))\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            loss_total += loss.item()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            \n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all,\n",
    "                                               predict_all,\n",
    "                                               target_names=config.label_list,\n",
    "                                               digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    return acc, loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class config:\n",
    "    current_path = \"\"\n",
    "    root_path = \"\"\n",
    "\n",
    "    data_path = \"/kaggle/input/thucnews/THUCNews/THUCNews\"\n",
    "\n",
    "    train_path = \"/kaggle/input/download/Downloads/train.csv\"\n",
    "    test_path = \"/kaggle/input/download/Downloads/test.csv\"\n",
    "    valid_path = \"/kaggle/input/download/Downloads/valid.csv\"\n",
    "\n",
    "    label_path = \"/kaggle/input/download/Downloads/label2id.json\"\n",
    "\n",
    "\n",
    "    is_cuda = True\n",
    "    device = torch.device('cuda') if is_cuda else torch.device('cpu')\n",
    "\n",
    "    \"\"\"with open(root_path + '/data/stopwords.txt', \"r\", encoding='utf-8') as f:\n",
    "        stopWords = [word.strip() for word in f.readlines()]\n",
    "    \"\"\"\n",
    "\n",
    "    with open(label_path, 'r', encoding='utf-8') as f:\n",
    "        label2id = json.load(f)\n",
    "\n",
    "    label_list = label2id.keys()\n",
    "    save_path = 'bert_wo_cls.pt'\n",
    "    # bert\n",
    "    eps = 1e-8\n",
    "    learning_rate = 2e-5  # 学习率\n",
    "    embedding_pretrained = None\n",
    "    batch_size = 64\n",
    "    hidden_size = 768\n",
    "    num_epochs = 20\n",
    "    dropout = 0.3  # 随机失活\n",
    "    require_improvement = 1000 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "    num_classes = len(label2id)  # 类别数\n",
    "    n_vocab = 50000  # 词表大小，在运行时赋值\n",
    "    embed = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.status.idle": "2023-02-06T07:07:49.669241Z",
     "shell.execute_reply": "2023-02-06T07:07:49.668233Z",
     "shell.execute_reply.started": "2023-02-06T06:43:43.019891Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "516it [01:28,  5.90it/s]\n",
      "  0%|          | 0/980 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/980 [00:00<01:06, 14.79it/s]\u001b[A\n",
      "  0%|          | 4/980 [00:00<01:04, 15.07it/s]\u001b[A\n",
      "  1%|          | 6/980 [00:00<01:04, 15.02it/s]\u001b[A\n",
      "  1%|          | 8/980 [00:00<01:05, 14.74it/s]\u001b[A\n",
      "  1%|          | 10/980 [00:00<01:05, 14.73it/s]\u001b[A\n",
      "  1%|          | 12/980 [00:00<01:05, 14.72it/s]\u001b[A\n",
      "  1%|▏         | 14/980 [00:00<01:05, 14.81it/s]\u001b[A\n",
      "  2%|▏         | 16/980 [00:01<01:05, 14.80it/s]\u001b[A\n",
      "  2%|▏         | 18/980 [00:01<01:04, 14.87it/s]\u001b[A\n",
      "  2%|▏         | 20/980 [00:01<01:04, 14.86it/s]\u001b[A\n",
      "  2%|▏         | 22/980 [00:01<01:04, 14.92it/s]\u001b[A\n",
      "  2%|▏         | 24/980 [00:01<01:02, 15.18it/s]\u001b[A\n",
      "  3%|▎         | 26/980 [00:01<01:02, 15.33it/s]\u001b[A\n",
      "  3%|▎         | 28/980 [00:01<01:02, 15.18it/s]\u001b[A\n",
      "  3%|▎         | 30/980 [00:02<01:02, 15.11it/s]\u001b[A\n",
      "  3%|▎         | 32/980 [00:02<01:02, 15.27it/s]\u001b[A\n",
      "  3%|▎         | 34/980 [00:02<01:02, 15.16it/s]\u001b[A\n",
      "  4%|▎         | 36/980 [00:02<01:01, 15.43it/s]\u001b[A\n",
      "  4%|▍         | 38/980 [00:02<01:01, 15.33it/s]\u001b[A\n",
      "  4%|▍         | 40/980 [00:02<01:01, 15.22it/s]\u001b[A\n",
      "  4%|▍         | 42/980 [00:02<01:01, 15.27it/s]\u001b[A\n",
      "  4%|▍         | 44/980 [00:02<01:02, 15.08it/s]\u001b[A\n",
      "  5%|▍         | 46/980 [00:03<01:02, 14.96it/s]\u001b[A\n",
      "  5%|▍         | 48/980 [00:03<01:03, 14.70it/s]\u001b[A\n",
      "  5%|▌         | 50/980 [00:03<01:03, 14.71it/s]\u001b[A\n",
      "  5%|▌         | 52/980 [00:03<01:02, 14.85it/s]\u001b[A\n",
      "  6%|▌         | 54/980 [00:03<01:02, 14.88it/s]\u001b[A\n",
      "  6%|▌         | 56/980 [00:03<01:03, 14.57it/s]\u001b[A\n",
      "  6%|▌         | 58/980 [00:03<01:03, 14.61it/s]\u001b[A\n",
      "  6%|▌         | 60/980 [00:04<01:03, 14.54it/s]\u001b[A\n",
      "  6%|▋         | 62/980 [00:04<01:03, 14.51it/s]\u001b[A\n",
      "  7%|▋         | 64/980 [00:04<01:02, 14.57it/s]\u001b[A\n",
      "  7%|▋         | 66/980 [00:04<01:02, 14.69it/s]\u001b[A\n",
      "  7%|▋         | 68/980 [00:04<01:02, 14.66it/s]\u001b[A\n",
      "  7%|▋         | 70/980 [00:04<01:01, 14.86it/s]\u001b[A\n",
      "  7%|▋         | 72/980 [00:04<01:01, 14.79it/s]\u001b[A\n",
      "  8%|▊         | 74/980 [00:04<01:00, 14.86it/s]\u001b[A\n",
      "  8%|▊         | 76/980 [00:05<01:00, 14.88it/s]\u001b[A\n",
      "  8%|▊         | 78/980 [00:05<01:01, 14.76it/s]\u001b[A\n",
      "  8%|▊         | 80/980 [00:05<01:00, 14.77it/s]\u001b[A\n",
      "  8%|▊         | 82/980 [00:05<01:00, 14.94it/s]\u001b[A\n",
      "  9%|▊         | 84/980 [00:05<01:00, 14.88it/s]\u001b[A\n",
      "  9%|▉         | 86/980 [00:05<00:59, 15.00it/s]\u001b[A\n",
      "  9%|▉         | 88/980 [00:05<00:59, 15.03it/s]\u001b[A\n",
      "  9%|▉         | 90/980 [00:06<00:59, 15.03it/s]\u001b[A\n",
      "  9%|▉         | 92/980 [00:06<00:59, 15.01it/s]\u001b[A\n",
      " 10%|▉         | 94/980 [00:06<00:59, 14.99it/s]\u001b[A\n",
      " 10%|▉         | 96/980 [00:06<00:59, 14.84it/s]\u001b[A\n",
      " 10%|█         | 98/980 [00:06<01:00, 14.65it/s]\u001b[A\n",
      " 10%|█         | 100/980 [00:06<00:59, 14.83it/s]\u001b[A\n",
      " 10%|█         | 102/980 [00:06<00:58, 15.03it/s]\u001b[A\n",
      " 11%|█         | 104/980 [00:06<00:57, 15.30it/s]\u001b[A\n",
      " 11%|█         | 106/980 [00:07<01:00, 14.36it/s]\u001b[A\n",
      " 11%|█         | 108/980 [00:07<01:05, 13.36it/s]\u001b[A\n",
      " 11%|█         | 110/980 [00:07<01:06, 13.09it/s]\u001b[A\n",
      " 11%|█▏        | 112/980 [00:07<01:05, 13.25it/s]\u001b[A\n",
      " 12%|█▏        | 114/980 [00:07<01:03, 13.64it/s]\u001b[A\n",
      " 12%|█▏        | 116/980 [00:07<01:01, 14.07it/s]\u001b[A\n",
      " 12%|█▏        | 118/980 [00:08<00:59, 14.41it/s]\u001b[A\n",
      " 12%|█▏        | 120/980 [00:08<01:00, 14.33it/s]\u001b[A\n",
      " 12%|█▏        | 122/980 [00:08<00:58, 14.55it/s]\u001b[A\n",
      " 13%|█▎        | 124/980 [00:08<00:58, 14.64it/s]\u001b[A\n",
      " 13%|█▎        | 126/980 [00:08<00:58, 14.69it/s]\u001b[A\n",
      " 13%|█▎        | 128/980 [00:08<00:57, 14.72it/s]\u001b[A\n",
      " 13%|█▎        | 130/980 [00:08<00:57, 14.79it/s]\u001b[A\n",
      " 13%|█▎        | 132/980 [00:08<00:56, 14.89it/s]\u001b[A\n",
      " 14%|█▎        | 134/980 [00:09<00:56, 14.88it/s]\u001b[A\n",
      " 14%|█▍        | 136/980 [00:09<00:56, 15.03it/s]\u001b[A\n",
      " 14%|█▍        | 138/980 [00:09<00:56, 14.87it/s]\u001b[A\n",
      " 14%|█▍        | 140/980 [00:09<00:56, 14.86it/s]\u001b[A\n",
      " 14%|█▍        | 142/980 [00:09<00:56, 14.92it/s]\u001b[A\n",
      " 15%|█▍        | 144/980 [00:09<00:55, 15.08it/s]\u001b[A\n",
      " 15%|█▍        | 146/980 [00:09<00:55, 14.99it/s]\u001b[A\n",
      " 15%|█▌        | 148/980 [00:10<00:55, 14.97it/s]\u001b[A\n",
      " 15%|█▌        | 150/980 [00:10<00:55, 15.04it/s]\u001b[A\n",
      " 16%|█▌        | 152/980 [00:10<00:54, 15.19it/s]\u001b[A\n",
      " 16%|█▌        | 154/980 [00:10<00:54, 15.24it/s]\u001b[A\n",
      " 16%|█▌        | 156/980 [00:10<00:53, 15.43it/s]\u001b[A\n",
      " 16%|█▌        | 158/980 [00:10<00:54, 15.17it/s]\u001b[A\n",
      " 16%|█▋        | 160/980 [00:10<00:54, 15.07it/s]\u001b[A\n",
      " 17%|█▋        | 162/980 [00:10<00:54, 15.04it/s]\u001b[A\n",
      " 17%|█▋        | 164/980 [00:11<00:54, 15.02it/s]\u001b[A\n",
      " 17%|█▋        | 166/980 [00:11<00:54, 15.06it/s]\u001b[A\n",
      " 17%|█▋        | 168/980 [00:11<00:54, 15.00it/s]\u001b[A\n",
      " 17%|█▋        | 170/980 [00:11<00:54, 14.96it/s]\u001b[A\n",
      " 18%|█▊        | 172/980 [00:11<00:54, 14.91it/s]\u001b[A\n",
      " 18%|█▊        | 174/980 [00:11<00:53, 14.93it/s]\u001b[A\n",
      " 18%|█▊        | 176/980 [00:11<00:53, 14.97it/s]\u001b[A\n",
      " 18%|█▊        | 178/980 [00:12<00:53, 14.95it/s]\u001b[A\n",
      " 18%|█▊        | 180/980 [00:12<00:53, 14.97it/s]\u001b[A\n",
      " 19%|█▊        | 182/980 [00:12<00:52, 15.21it/s]\u001b[A\n",
      " 19%|█▉        | 184/980 [00:12<00:52, 15.29it/s]\u001b[A\n",
      " 19%|█▉        | 186/980 [00:12<00:52, 15.20it/s]\u001b[A\n",
      " 19%|█▉        | 188/980 [00:12<00:52, 15.09it/s]\u001b[A\n",
      " 19%|█▉        | 190/980 [00:12<00:53, 14.90it/s]\u001b[A\n",
      " 20%|█▉        | 192/980 [00:12<00:52, 15.07it/s]\u001b[A\n",
      " 20%|█▉        | 194/980 [00:13<00:53, 14.76it/s]\u001b[A\n",
      " 20%|██        | 196/980 [00:13<00:52, 14.82it/s]\u001b[A\n",
      " 20%|██        | 198/980 [00:13<00:52, 15.02it/s]\u001b[A\n",
      " 20%|██        | 200/980 [00:13<00:51, 15.16it/s]\u001b[A\n",
      " 21%|██        | 202/980 [00:13<00:50, 15.41it/s]\u001b[A\n",
      " 21%|██        | 204/980 [00:13<00:50, 15.26it/s]\u001b[A\n",
      " 21%|██        | 206/980 [00:13<00:50, 15.19it/s]\u001b[A\n",
      " 21%|██        | 208/980 [00:13<00:50, 15.17it/s]\u001b[A\n",
      " 21%|██▏       | 210/980 [00:14<00:50, 15.32it/s]\u001b[A\n",
      " 22%|██▏       | 212/980 [00:14<00:50, 15.26it/s]\u001b[A\n",
      " 22%|██▏       | 214/980 [00:14<00:50, 15.13it/s]\u001b[A\n",
      " 22%|██▏       | 216/980 [00:14<00:51, 14.88it/s]\u001b[A\n",
      " 22%|██▏       | 218/980 [00:14<00:51, 14.77it/s]\u001b[A\n",
      " 22%|██▏       | 220/980 [00:14<00:51, 14.80it/s]\u001b[A\n",
      " 23%|██▎       | 222/980 [00:14<00:50, 14.97it/s]\u001b[A\n",
      " 23%|██▎       | 224/980 [00:15<00:49, 15.13it/s]\u001b[A\n",
      " 23%|██▎       | 226/980 [00:15<00:49, 15.14it/s]\u001b[A\n",
      " 23%|██▎       | 228/980 [00:15<00:49, 15.13it/s]\u001b[A\n",
      " 23%|██▎       | 230/980 [00:15<00:49, 15.02it/s]\u001b[A\n",
      " 24%|██▎       | 232/980 [00:15<00:49, 15.17it/s]\u001b[A\n",
      " 24%|██▍       | 234/980 [00:15<00:49, 15.14it/s]\u001b[A\n",
      " 24%|██▍       | 236/980 [00:15<00:49, 15.00it/s]\u001b[A\n",
      " 24%|██▍       | 238/980 [00:16<00:51, 14.45it/s]\u001b[A\n",
      " 24%|██▍       | 240/980 [00:16<00:51, 14.31it/s]\u001b[A\n",
      " 25%|██▍       | 242/980 [00:16<00:50, 14.55it/s]\u001b[A\n",
      " 25%|██▍       | 244/980 [00:16<00:49, 14.75it/s]\u001b[A\n",
      " 25%|██▌       | 246/980 [00:16<00:49, 14.77it/s]\u001b[A\n",
      " 25%|██▌       | 248/980 [00:16<00:49, 14.90it/s]\u001b[A\n",
      " 26%|██▌       | 250/980 [00:16<00:49, 14.90it/s]\u001b[A\n",
      " 26%|██▌       | 252/980 [00:16<00:48, 14.95it/s]\u001b[A\n",
      " 26%|██▌       | 254/980 [00:17<00:49, 14.76it/s]\u001b[A\n",
      " 26%|██▌       | 256/980 [00:17<00:48, 14.98it/s]\u001b[A\n",
      " 26%|██▋       | 258/980 [00:17<00:48, 14.94it/s]\u001b[A\n",
      " 27%|██▋       | 260/980 [00:17<00:49, 14.58it/s]\u001b[A\n",
      " 27%|██▋       | 262/980 [00:17<00:48, 14.71it/s]\u001b[A\n",
      " 27%|██▋       | 264/980 [00:17<00:48, 14.83it/s]\u001b[A\n",
      " 27%|██▋       | 266/980 [00:17<00:48, 14.78it/s]\u001b[A\n",
      " 27%|██▋       | 268/980 [00:18<00:47, 14.87it/s]\u001b[A\n",
      " 28%|██▊       | 270/980 [00:18<00:52, 13.61it/s]\u001b[A\n",
      " 28%|██▊       | 272/980 [00:18<00:53, 13.22it/s]\u001b[A\n",
      " 28%|██▊       | 274/980 [00:18<00:53, 13.19it/s]\u001b[A\n",
      " 28%|██▊       | 276/980 [00:18<00:52, 13.41it/s]\u001b[A\n",
      " 28%|██▊       | 278/980 [00:18<00:51, 13.68it/s]\u001b[A\n",
      " 29%|██▊       | 280/980 [00:18<00:49, 14.08it/s]\u001b[A\n",
      " 29%|██▉       | 282/980 [00:19<00:48, 14.41it/s]\u001b[A\n",
      " 29%|██▉       | 284/980 [00:19<00:47, 14.60it/s]\u001b[A\n",
      " 29%|██▉       | 286/980 [00:19<00:47, 14.75it/s]\u001b[A\n",
      " 29%|██▉       | 288/980 [00:19<00:46, 14.74it/s]\u001b[A\n",
      " 30%|██▉       | 290/980 [00:19<00:46, 14.83it/s]\u001b[A\n",
      " 30%|██▉       | 292/980 [00:19<00:46, 14.80it/s]\u001b[A\n",
      " 30%|███       | 294/980 [00:19<00:46, 14.89it/s]\u001b[A\n",
      " 30%|███       | 296/980 [00:19<00:45, 14.96it/s]\u001b[A\n",
      " 30%|███       | 298/980 [00:20<00:45, 15.03it/s]\u001b[A\n",
      " 31%|███       | 300/980 [00:20<00:45, 14.83it/s]\u001b[A\n",
      " 31%|███       | 302/980 [00:20<00:45, 14.93it/s]\u001b[A\n",
      " 31%|███       | 304/980 [00:20<00:45, 14.99it/s]\u001b[A\n",
      " 31%|███       | 306/980 [00:20<00:44, 15.27it/s]\u001b[A\n",
      " 31%|███▏      | 308/980 [00:20<00:44, 15.14it/s]\u001b[A\n",
      " 32%|███▏      | 310/980 [00:20<00:43, 15.28it/s]\u001b[A\n",
      " 32%|███▏      | 312/980 [00:21<00:43, 15.21it/s]\u001b[A\n",
      " 32%|███▏      | 314/980 [00:21<00:43, 15.20it/s]\u001b[A\n",
      " 32%|███▏      | 316/980 [00:21<00:44, 14.96it/s]\u001b[A\n",
      " 32%|███▏      | 318/980 [00:21<00:44, 15.00it/s]\u001b[A\n",
      " 33%|███▎      | 320/980 [00:21<00:43, 15.14it/s]\u001b[A\n",
      " 33%|███▎      | 322/980 [00:21<00:43, 15.27it/s]\u001b[A\n",
      " 33%|███▎      | 324/980 [00:21<00:43, 15.16it/s]\u001b[A\n",
      " 33%|███▎      | 326/980 [00:21<00:43, 15.20it/s]\u001b[A\n",
      " 33%|███▎      | 328/980 [00:22<00:43, 15.01it/s]\u001b[A\n",
      " 34%|███▎      | 330/980 [00:22<00:43, 14.96it/s]\u001b[A\n",
      " 34%|███▍      | 332/980 [00:22<00:42, 15.11it/s]\u001b[A\n",
      " 34%|███▍      | 334/980 [00:22<00:42, 15.25it/s]\u001b[A\n",
      " 34%|███▍      | 336/980 [00:22<00:42, 15.16it/s]\u001b[A\n",
      " 34%|███▍      | 338/980 [00:22<00:42, 14.95it/s]\u001b[A\n",
      " 35%|███▍      | 340/980 [00:22<00:45, 14.00it/s]\u001b[A\n",
      " 35%|███▍      | 342/980 [00:23<00:47, 13.50it/s]\u001b[A\n",
      " 35%|███▌      | 344/980 [00:23<00:48, 13.24it/s]\u001b[A\n",
      " 35%|███▌      | 346/980 [00:23<00:47, 13.23it/s]\u001b[A\n",
      " 36%|███▌      | 348/980 [00:23<00:47, 13.26it/s]\u001b[A\n",
      " 36%|███▌      | 350/980 [00:23<00:47, 13.29it/s]\u001b[A\n",
      " 36%|███▌      | 352/980 [00:23<00:48, 13.06it/s]\u001b[A\n",
      " 36%|███▌      | 354/980 [00:24<00:46, 13.56it/s]\u001b[A\n",
      " 36%|███▋      | 356/980 [00:24<00:44, 13.93it/s]\u001b[A\n",
      " 37%|███▋      | 358/980 [00:24<00:43, 14.22it/s]\u001b[A\n",
      " 37%|███▋      | 360/980 [00:24<00:43, 14.25it/s]\u001b[A\n",
      " 37%|███▋      | 362/980 [00:24<00:42, 14.59it/s]\u001b[A\n",
      " 37%|███▋      | 364/980 [00:24<00:41, 14.76it/s]\u001b[A\n",
      " 37%|███▋      | 366/980 [00:24<00:41, 14.64it/s]\u001b[A\n",
      " 38%|███▊      | 368/980 [00:24<00:41, 14.63it/s]\u001b[A\n",
      " 38%|███▊      | 370/980 [00:25<00:41, 14.54it/s]\u001b[A\n",
      " 38%|███▊      | 372/980 [00:25<00:41, 14.74it/s]\u001b[A\n",
      " 38%|███▊      | 374/980 [00:25<00:41, 14.77it/s]\u001b[A\n",
      " 38%|███▊      | 376/980 [00:25<00:40, 14.82it/s]\u001b[A\n",
      " 39%|███▊      | 378/980 [00:25<00:40, 14.89it/s]\u001b[A\n",
      " 39%|███▉      | 380/980 [00:25<00:40, 14.85it/s]\u001b[A\n",
      " 39%|███▉      | 382/980 [00:25<00:40, 14.81it/s]\u001b[A\n",
      " 39%|███▉      | 384/980 [00:26<00:40, 14.82it/s]\u001b[A\n",
      " 39%|███▉      | 386/980 [00:26<00:40, 14.84it/s]\u001b[A\n",
      " 40%|███▉      | 388/980 [00:26<00:39, 14.86it/s]\u001b[A\n",
      " 40%|███▉      | 390/980 [00:26<00:39, 14.93it/s]\u001b[A\n",
      " 40%|████      | 392/980 [00:26<00:38, 15.12it/s]\u001b[A\n",
      " 40%|████      | 394/980 [00:26<00:38, 15.07it/s]\u001b[A\n",
      " 40%|████      | 396/980 [00:26<00:38, 15.09it/s]\u001b[A\n",
      " 41%|████      | 398/980 [00:26<00:38, 15.21it/s]\u001b[A\n",
      " 41%|████      | 400/980 [00:27<00:38, 15.05it/s]\u001b[A\n",
      " 41%|████      | 402/980 [00:27<00:38, 15.07it/s]\u001b[A\n",
      " 41%|████      | 404/980 [00:27<00:38, 15.11it/s]\u001b[A\n",
      " 41%|████▏     | 406/980 [00:27<00:38, 15.08it/s]\u001b[A\n",
      " 42%|████▏     | 408/980 [00:27<00:38, 14.99it/s]\u001b[A\n",
      " 42%|████▏     | 410/980 [00:27<00:38, 14.96it/s]\u001b[A\n",
      " 42%|████▏     | 412/980 [00:27<00:38, 14.87it/s]\u001b[A\n",
      " 42%|████▏     | 414/980 [00:28<00:37, 14.90it/s]\u001b[A\n",
      " 42%|████▏     | 416/980 [00:28<00:38, 14.82it/s]\u001b[A\n",
      " 43%|████▎     | 418/980 [00:28<00:37, 15.12it/s]\u001b[A\n",
      " 43%|████▎     | 420/980 [00:28<00:37, 15.03it/s]\u001b[A\n",
      " 43%|████▎     | 422/980 [00:28<00:36, 15.22it/s]\u001b[A\n",
      " 43%|████▎     | 424/980 [00:28<00:36, 15.19it/s]\u001b[A\n",
      " 43%|████▎     | 426/980 [00:28<00:36, 15.39it/s]\u001b[A\n",
      " 44%|████▎     | 428/980 [00:28<00:36, 15.32it/s]\u001b[A\n",
      " 44%|████▍     | 430/980 [00:29<00:38, 14.32it/s]\u001b[A\n",
      " 44%|████▍     | 432/980 [00:29<00:40, 13.59it/s]\u001b[A\n",
      " 44%|████▍     | 434/980 [00:29<00:41, 13.17it/s]\u001b[A\n",
      " 44%|████▍     | 436/980 [00:29<00:41, 13.04it/s]\u001b[A\n",
      " 45%|████▍     | 438/980 [00:29<00:39, 13.58it/s]\u001b[A\n",
      " 45%|████▍     | 440/980 [00:29<00:38, 13.99it/s]\u001b[A\n",
      " 45%|████▌     | 442/980 [00:29<00:37, 14.26it/s]\u001b[A\n",
      " 45%|████▌     | 444/980 [00:30<00:36, 14.59it/s]\u001b[A\n",
      " 46%|████▌     | 446/980 [00:30<00:35, 14.86it/s]\u001b[A\n",
      " 46%|████▌     | 448/980 [00:30<00:35, 14.91it/s]\u001b[A\n",
      " 46%|████▌     | 450/980 [00:30<00:35, 14.91it/s]\u001b[A\n",
      " 46%|████▌     | 452/980 [00:30<00:35, 15.03it/s]\u001b[A\n",
      " 46%|████▋     | 454/980 [00:30<00:35, 14.97it/s]\u001b[A\n",
      " 47%|████▋     | 456/980 [00:30<00:34, 15.12it/s]\u001b[A\n",
      " 47%|████▋     | 458/980 [00:31<00:34, 15.17it/s]\u001b[A\n",
      " 47%|████▋     | 460/980 [00:31<00:34, 15.25it/s]\u001b[A\n",
      " 47%|████▋     | 462/980 [00:31<00:33, 15.36it/s]\u001b[A\n",
      " 47%|████▋     | 464/980 [00:31<00:33, 15.22it/s]\u001b[A\n",
      " 48%|████▊     | 466/980 [00:31<00:33, 15.15it/s]\u001b[A\n",
      " 48%|████▊     | 468/980 [00:31<00:33, 15.24it/s]\u001b[A\n",
      " 48%|████▊     | 470/980 [00:31<00:33, 15.24it/s]\u001b[A\n",
      " 48%|████▊     | 472/980 [00:31<00:33, 15.22it/s]\u001b[A\n",
      " 48%|████▊     | 474/980 [00:32<00:33, 15.09it/s]\u001b[A\n",
      " 49%|████▊     | 476/980 [00:32<00:33, 15.16it/s]\u001b[A\n",
      " 49%|████▉     | 478/980 [00:32<00:33, 14.98it/s]\u001b[A\n",
      " 49%|████▉     | 480/980 [00:32<00:33, 15.00it/s]\u001b[A\n",
      " 49%|████▉     | 482/980 [00:32<00:33, 14.89it/s]\u001b[A\n",
      " 49%|████▉     | 484/980 [00:32<00:33, 15.00it/s]\u001b[A\n",
      " 50%|████▉     | 486/980 [00:32<00:33, 14.86it/s]\u001b[A\n",
      " 50%|████▉     | 488/980 [00:33<00:32, 15.11it/s]\u001b[A\n",
      " 50%|█████     | 490/980 [00:33<00:32, 15.08it/s]\u001b[A\n",
      " 50%|█████     | 492/980 [00:33<00:32, 14.79it/s]\u001b[A\n",
      " 50%|█████     | 494/980 [00:33<00:32, 14.91it/s]\u001b[A\n",
      " 51%|█████     | 496/980 [00:33<00:32, 15.10it/s]\u001b[A\n",
      " 51%|█████     | 498/980 [00:33<00:32, 14.97it/s]\u001b[A\n",
      " 51%|█████     | 500/980 [00:33<00:32, 14.87it/s]\u001b[A\n",
      " 51%|█████     | 502/980 [00:33<00:31, 15.07it/s]\u001b[A\n",
      " 51%|█████▏    | 504/980 [00:34<00:31, 14.97it/s]\u001b[A\n",
      " 52%|█████▏    | 506/980 [00:34<00:31, 14.83it/s]\u001b[A\n",
      " 52%|█████▏    | 508/980 [00:34<00:31, 14.77it/s]\u001b[A\n",
      " 52%|█████▏    | 510/980 [00:34<00:32, 14.61it/s]\u001b[A\n",
      " 52%|█████▏    | 512/980 [00:34<00:32, 14.61it/s]\u001b[A\n",
      " 52%|█████▏    | 514/980 [00:34<00:31, 14.84it/s]\u001b[A\n",
      " 53%|█████▎    | 516/980 [00:34<00:31, 14.71it/s]\u001b[A\n",
      " 53%|█████▎    | 518/980 [00:35<00:31, 14.74it/s]\u001b[A\n",
      " 53%|█████▎    | 520/980 [00:35<00:30, 14.86it/s]\u001b[A\n",
      " 53%|█████▎    | 522/980 [00:35<00:30, 14.84it/s]\u001b[A\n",
      " 53%|█████▎    | 524/980 [00:35<00:30, 14.92it/s]\u001b[A\n",
      " 54%|█████▎    | 526/980 [00:35<00:30, 14.97it/s]\u001b[A\n",
      " 54%|█████▍    | 528/980 [00:35<00:30, 14.90it/s]\u001b[A\n",
      " 54%|█████▍    | 530/980 [00:35<00:30, 14.90it/s]\u001b[A\n",
      " 54%|█████▍    | 532/980 [00:35<00:29, 14.94it/s]\u001b[A\n",
      " 54%|█████▍    | 534/980 [00:36<00:30, 14.55it/s]\u001b[A\n",
      " 55%|█████▍    | 536/980 [00:36<00:30, 14.74it/s]\u001b[A\n",
      " 55%|█████▍    | 538/980 [00:36<00:30, 14.71it/s]\u001b[A\n",
      " 55%|█████▌    | 540/980 [00:36<00:29, 14.80it/s]\u001b[A\n",
      " 55%|█████▌    | 542/980 [00:36<00:29, 14.92it/s]\u001b[A\n",
      " 56%|█████▌    | 544/980 [00:36<00:29, 14.95it/s]\u001b[A\n",
      " 56%|█████▌    | 546/980 [00:36<00:29, 14.82it/s]\u001b[A\n",
      " 56%|█████▌    | 548/980 [00:37<00:29, 14.82it/s]\u001b[A\n",
      " 56%|█████▌    | 550/980 [00:37<00:28, 14.92it/s]\u001b[A\n",
      " 56%|█████▋    | 552/980 [00:37<00:28, 14.97it/s]\u001b[A\n",
      " 57%|█████▋    | 554/980 [00:37<00:28, 15.11it/s]\u001b[A\n",
      " 57%|█████▋    | 556/980 [00:37<00:28, 14.99it/s]\u001b[A\n",
      " 57%|█████▋    | 558/980 [00:37<00:27, 15.07it/s]\u001b[A\n",
      " 57%|█████▋    | 560/980 [00:37<00:27, 15.07it/s]\u001b[A\n",
      " 57%|█████▋    | 562/980 [00:38<00:27, 15.00it/s]\u001b[A\n",
      " 58%|█████▊    | 564/980 [00:38<00:27, 15.10it/s]\u001b[A\n",
      " 58%|█████▊    | 566/980 [00:38<00:27, 15.02it/s]\u001b[A\n",
      " 58%|█████▊    | 568/980 [00:38<00:27, 14.95it/s]\u001b[A\n",
      " 58%|█████▊    | 570/980 [00:38<00:27, 15.01it/s]\u001b[A\n",
      " 58%|█████▊    | 572/980 [00:38<00:27, 15.09it/s]\u001b[A\n",
      " 59%|█████▊    | 574/980 [00:38<00:27, 14.87it/s]\u001b[A\n",
      " 59%|█████▉    | 576/980 [00:38<00:26, 15.00it/s]\u001b[A\n",
      " 59%|█████▉    | 578/980 [00:39<00:26, 15.00it/s]\u001b[A\n",
      " 59%|█████▉    | 580/980 [00:39<00:26, 14.92it/s]\u001b[A\n",
      " 59%|█████▉    | 582/980 [00:39<00:26, 14.99it/s]\u001b[A\n",
      " 60%|█████▉    | 584/980 [00:39<00:26, 15.04it/s]\u001b[A\n",
      " 60%|█████▉    | 586/980 [00:39<00:26, 14.78it/s]\u001b[A\n",
      " 60%|██████    | 588/980 [00:39<00:26, 14.82it/s]\u001b[A\n",
      " 60%|██████    | 590/980 [00:39<00:26, 14.89it/s]\u001b[A\n",
      " 60%|██████    | 592/980 [00:40<00:26, 14.80it/s]\u001b[A\n",
      " 61%|██████    | 594/980 [00:40<00:28, 13.39it/s]\u001b[A\n",
      " 61%|██████    | 596/980 [00:40<00:28, 13.32it/s]\u001b[A\n",
      " 61%|██████    | 598/980 [00:40<00:29, 13.16it/s]\u001b[A\n",
      " 61%|██████    | 600/980 [00:40<00:29, 13.10it/s]\u001b[A\n",
      " 61%|██████▏   | 602/980 [00:40<00:27, 13.69it/s]\u001b[A\n",
      " 62%|██████▏   | 604/980 [00:40<00:26, 14.09it/s]\u001b[A\n",
      " 62%|██████▏   | 606/980 [00:41<00:26, 14.29it/s]\u001b[A\n",
      " 62%|██████▏   | 608/980 [00:41<00:25, 14.48it/s]\u001b[A\n",
      " 62%|██████▏   | 610/980 [00:41<00:24, 14.83it/s]\u001b[A\n",
      " 62%|██████▏   | 612/980 [00:41<00:25, 14.65it/s]\u001b[A\n",
      " 63%|██████▎   | 614/980 [00:41<00:24, 14.70it/s]\u001b[A\n",
      " 63%|██████▎   | 616/980 [00:41<00:24, 14.70it/s]\u001b[A\n",
      " 63%|██████▎   | 618/980 [00:41<00:24, 14.74it/s]\u001b[A\n",
      " 63%|██████▎   | 620/980 [00:41<00:24, 14.93it/s]\u001b[A\n",
      " 63%|██████▎   | 622/980 [00:42<00:24, 14.58it/s]\u001b[A\n",
      " 64%|██████▎   | 624/980 [00:42<00:24, 14.63it/s]\u001b[A\n",
      " 64%|██████▍   | 626/980 [00:42<00:23, 14.83it/s]\u001b[A\n",
      " 64%|██████▍   | 628/980 [00:42<00:23, 14.91it/s]\u001b[A\n",
      " 64%|██████▍   | 630/980 [00:42<00:23, 14.65it/s]\u001b[A\n",
      " 64%|██████▍   | 632/980 [00:42<00:23, 14.98it/s]\u001b[A\n",
      " 65%|██████▍   | 634/980 [00:42<00:23, 14.98it/s]\u001b[A\n",
      " 65%|██████▍   | 636/980 [00:43<00:22, 15.14it/s]\u001b[A\n",
      " 65%|██████▌   | 638/980 [00:43<00:22, 15.15it/s]\u001b[A\n",
      " 65%|██████▌   | 640/980 [00:43<00:22, 15.15it/s]\u001b[A\n",
      " 66%|██████▌   | 642/980 [00:43<00:22, 15.09it/s]\u001b[A\n",
      " 66%|██████▌   | 644/980 [00:43<00:22, 14.93it/s]\u001b[A\n",
      " 66%|██████▌   | 646/980 [00:43<00:22, 14.95it/s]\u001b[A\n",
      " 66%|██████▌   | 648/980 [00:43<00:22, 15.01it/s]\u001b[A\n",
      " 66%|██████▋   | 650/980 [00:44<00:22, 14.84it/s]\u001b[A\n",
      " 67%|██████▋   | 652/980 [00:44<00:21, 14.92it/s]\u001b[A\n",
      " 67%|██████▋   | 654/980 [00:44<00:21, 15.07it/s]\u001b[A\n",
      " 67%|██████▋   | 656/980 [00:44<00:21, 15.05it/s]\u001b[A\n",
      " 67%|██████▋   | 658/980 [00:44<00:21, 15.02it/s]\u001b[A\n",
      " 67%|██████▋   | 660/980 [00:44<00:21, 15.14it/s]\u001b[A\n",
      " 68%|██████▊   | 662/980 [00:44<00:21, 15.04it/s]\u001b[A\n",
      " 68%|██████▊   | 664/980 [00:44<00:21, 15.03it/s]\u001b[A\n",
      " 68%|██████▊   | 666/980 [00:45<00:21, 14.85it/s]\u001b[A\n",
      " 68%|██████▊   | 668/980 [00:45<00:20, 14.98it/s]\u001b[A\n",
      " 68%|██████▊   | 670/980 [00:45<00:20, 14.90it/s]\u001b[A\n",
      " 69%|██████▊   | 672/980 [00:45<00:20, 14.93it/s]\u001b[A\n",
      " 69%|██████▉   | 674/980 [00:45<00:20, 14.92it/s]\u001b[A\n",
      " 69%|██████▉   | 676/980 [00:45<00:20, 14.96it/s]\u001b[A\n",
      " 69%|██████▉   | 678/980 [00:45<00:20, 15.02it/s]\u001b[A\n",
      " 69%|██████▉   | 680/980 [00:46<00:19, 15.20it/s]\u001b[A\n",
      " 70%|██████▉   | 682/980 [00:46<00:20, 14.36it/s]\u001b[A\n",
      " 70%|██████▉   | 684/980 [00:46<00:20, 14.54it/s]\u001b[A\n",
      " 70%|███████   | 686/980 [00:46<00:19, 14.71it/s]\u001b[A\n",
      " 70%|███████   | 688/980 [00:46<00:19, 14.66it/s]\u001b[A\n",
      " 70%|███████   | 690/980 [00:46<00:19, 14.78it/s]\u001b[A\n",
      " 71%|███████   | 692/980 [00:46<00:19, 14.79it/s]\u001b[A\n",
      " 71%|███████   | 694/980 [00:46<00:19, 14.94it/s]\u001b[A\n",
      " 71%|███████   | 696/980 [00:47<00:18, 15.14it/s]\u001b[A\n",
      " 71%|███████   | 698/980 [00:47<00:18, 15.05it/s]\u001b[A\n",
      " 71%|███████▏  | 700/980 [00:47<00:18, 14.97it/s]\u001b[A\n",
      " 72%|███████▏  | 702/980 [00:47<00:18, 15.16it/s]\u001b[A\n",
      " 72%|███████▏  | 704/980 [00:47<00:18, 15.27it/s]\u001b[A\n",
      " 72%|███████▏  | 706/980 [00:47<00:18, 15.21it/s]\u001b[A\n",
      " 72%|███████▏  | 708/980 [00:47<00:17, 15.31it/s]\u001b[A\n",
      " 72%|███████▏  | 710/980 [00:48<00:17, 15.21it/s]\u001b[A\n",
      " 73%|███████▎  | 712/980 [00:48<00:17, 15.23it/s]\u001b[A\n",
      " 73%|███████▎  | 714/980 [00:48<00:17, 15.22it/s]\u001b[A\n",
      " 73%|███████▎  | 716/980 [00:48<00:17, 15.10it/s]\u001b[A\n",
      " 73%|███████▎  | 718/980 [00:48<00:17, 15.22it/s]\u001b[A\n",
      " 73%|███████▎  | 720/980 [00:48<00:16, 15.31it/s]\u001b[A\n",
      " 74%|███████▎  | 722/980 [00:48<00:16, 15.20it/s]\u001b[A\n",
      " 74%|███████▍  | 724/980 [00:48<00:16, 15.25it/s]\u001b[A\n",
      " 74%|███████▍  | 726/980 [00:49<00:17, 14.88it/s]\u001b[A\n",
      " 74%|███████▍  | 728/980 [00:49<00:17, 14.16it/s]\u001b[A\n",
      " 74%|███████▍  | 730/980 [00:49<00:17, 14.10it/s]\u001b[A\n",
      " 75%|███████▍  | 732/980 [00:49<00:17, 14.33it/s]\u001b[A\n",
      " 75%|███████▍  | 734/980 [00:49<00:16, 14.67it/s]\u001b[A\n",
      " 75%|███████▌  | 736/980 [00:49<00:16, 14.71it/s]\u001b[A\n",
      " 75%|███████▌  | 738/980 [00:49<00:16, 14.66it/s]\u001b[A\n",
      " 76%|███████▌  | 740/980 [00:50<00:16, 14.60it/s]\u001b[A\n",
      " 76%|███████▌  | 742/980 [00:50<00:16, 14.81it/s]\u001b[A\n",
      " 76%|███████▌  | 744/980 [00:50<00:15, 14.88it/s]\u001b[A\n",
      " 76%|███████▌  | 746/980 [00:50<00:15, 14.92it/s]\u001b[A\n",
      " 76%|███████▋  | 748/980 [00:50<00:15, 14.91it/s]\u001b[A\n",
      " 77%|███████▋  | 750/980 [00:50<00:15, 14.90it/s]\u001b[A\n",
      " 77%|███████▋  | 752/980 [00:50<00:15, 14.93it/s]\u001b[A\n",
      " 77%|███████▋  | 754/980 [00:50<00:15, 15.02it/s]\u001b[A\n",
      " 77%|███████▋  | 756/980 [00:51<00:15, 14.28it/s]\u001b[A\n",
      " 77%|███████▋  | 758/980 [00:51<00:16, 13.69it/s]\u001b[A\n",
      " 78%|███████▊  | 760/980 [00:51<00:16, 13.42it/s]\u001b[A\n",
      " 78%|███████▊  | 762/980 [00:51<00:16, 13.10it/s]\u001b[A\n",
      " 78%|███████▊  | 764/980 [00:51<00:16, 13.47it/s]\u001b[A\n",
      " 78%|███████▊  | 766/980 [00:51<00:15, 13.91it/s]\u001b[A\n",
      " 78%|███████▊  | 768/980 [00:52<00:15, 14.12it/s]\u001b[A\n",
      " 79%|███████▊  | 770/980 [00:52<00:14, 14.54it/s]\u001b[A\n",
      " 79%|███████▉  | 772/980 [00:52<00:14, 14.73it/s]\u001b[A\n",
      " 79%|███████▉  | 774/980 [00:52<00:13, 14.95it/s]\u001b[A\n",
      " 79%|███████▉  | 776/980 [00:52<00:13, 14.95it/s]\u001b[A\n",
      " 79%|███████▉  | 778/980 [00:52<00:13, 14.92it/s]\u001b[A\n",
      " 80%|███████▉  | 780/980 [00:52<00:13, 14.90it/s]\u001b[A\n",
      " 80%|███████▉  | 782/980 [00:52<00:13, 15.14it/s]\u001b[A\n",
      " 80%|████████  | 784/980 [00:53<00:12, 15.23it/s]\u001b[A\n",
      " 80%|████████  | 786/980 [00:53<00:12, 15.10it/s]\u001b[A\n",
      " 80%|████████  | 788/980 [00:53<00:12, 15.03it/s]\u001b[A\n",
      " 81%|████████  | 790/980 [00:53<00:12, 14.90it/s]\u001b[A\n",
      " 81%|████████  | 792/980 [00:53<00:12, 14.91it/s]\u001b[A\n",
      " 81%|████████  | 794/980 [00:53<00:12, 15.09it/s]\u001b[A\n",
      " 81%|████████  | 796/980 [00:53<00:12, 15.12it/s]\u001b[A\n",
      " 81%|████████▏ | 798/980 [00:53<00:11, 15.26it/s]\u001b[A\n",
      " 82%|████████▏ | 800/980 [00:54<00:11, 15.22it/s]\u001b[A\n",
      " 82%|████████▏ | 802/980 [00:54<00:12, 14.39it/s]\u001b[A\n",
      " 82%|████████▏ | 804/980 [00:54<00:12, 13.85it/s]\u001b[A\n",
      " 82%|████████▏ | 806/980 [00:54<00:12, 13.43it/s]\u001b[A\n",
      " 82%|████████▏ | 808/980 [00:54<00:13, 13.23it/s]\u001b[A\n",
      " 83%|████████▎ | 810/980 [00:54<00:12, 13.25it/s]\u001b[A\n",
      " 83%|████████▎ | 812/980 [00:55<00:12, 13.28it/s]\u001b[A\n",
      " 83%|████████▎ | 814/980 [00:55<00:12, 12.99it/s]\u001b[A\n",
      " 83%|████████▎ | 816/980 [00:55<00:12, 13.01it/s]\u001b[A\n",
      " 83%|████████▎ | 818/980 [00:55<00:12, 13.23it/s]\u001b[A\n",
      " 84%|████████▎ | 820/980 [00:55<00:11, 13.72it/s]\u001b[A\n",
      " 84%|████████▍ | 822/980 [00:55<00:11, 14.04it/s]\u001b[A\n",
      " 84%|████████▍ | 824/980 [00:55<00:10, 14.41it/s]\u001b[A\n",
      " 84%|████████▍ | 826/980 [00:56<00:10, 14.32it/s]\u001b[A\n",
      " 84%|████████▍ | 828/980 [00:56<00:10, 14.66it/s]\u001b[A\n",
      " 85%|████████▍ | 830/980 [00:56<00:10, 14.62it/s]\u001b[A\n",
      " 85%|████████▍ | 832/980 [00:56<00:10, 14.74it/s]\u001b[A\n",
      " 85%|████████▌ | 834/980 [00:56<00:09, 14.73it/s]\u001b[A\n",
      " 85%|████████▌ | 836/980 [00:56<00:09, 14.78it/s]\u001b[A\n",
      " 86%|████████▌ | 838/980 [00:56<00:09, 15.07it/s]\u001b[A\n",
      " 86%|████████▌ | 840/980 [00:56<00:09, 15.02it/s]\u001b[A\n",
      " 86%|████████▌ | 842/980 [00:57<00:09, 15.08it/s]\u001b[A\n",
      " 86%|████████▌ | 844/980 [00:57<00:08, 15.20it/s]\u001b[A\n",
      " 86%|████████▋ | 846/980 [00:57<00:08, 15.13it/s]\u001b[A\n",
      " 87%|████████▋ | 848/980 [00:57<00:08, 15.03it/s]\u001b[A\n",
      " 87%|████████▋ | 850/980 [00:57<00:08, 15.19it/s]\u001b[A\n",
      " 87%|████████▋ | 852/980 [00:57<00:08, 15.31it/s]\u001b[A\n",
      " 87%|████████▋ | 854/980 [00:57<00:08, 15.26it/s]\u001b[A\n",
      " 87%|████████▋ | 856/980 [00:58<00:08, 15.15it/s]\u001b[A\n",
      " 88%|████████▊ | 858/980 [00:58<00:08, 15.07it/s]\u001b[A\n",
      " 88%|████████▊ | 860/980 [00:58<00:08, 14.91it/s]\u001b[A\n",
      " 88%|████████▊ | 862/980 [00:58<00:07, 15.05it/s]\u001b[A\n",
      " 88%|████████▊ | 864/980 [00:58<00:07, 15.03it/s]\u001b[A\n",
      " 88%|████████▊ | 866/980 [00:58<00:07, 15.07it/s]\u001b[A\n",
      " 89%|████████▊ | 868/980 [00:58<00:07, 15.04it/s]\u001b[A\n",
      " 89%|████████▉ | 870/980 [00:58<00:07, 15.17it/s]\u001b[A\n",
      " 89%|████████▉ | 872/980 [00:59<00:07, 15.17it/s]\u001b[A\n",
      " 89%|████████▉ | 874/980 [00:59<00:07, 15.08it/s]\u001b[A\n",
      " 89%|████████▉ | 876/980 [00:59<00:06, 15.19it/s]\u001b[A\n",
      " 90%|████████▉ | 878/980 [00:59<00:06, 15.10it/s]\u001b[A\n",
      " 90%|████████▉ | 880/980 [00:59<00:06, 15.09it/s]\u001b[A\n",
      " 90%|█████████ | 882/980 [00:59<00:06, 15.26it/s]\u001b[A\n",
      " 90%|█████████ | 884/980 [00:59<00:06, 15.24it/s]\u001b[A\n",
      " 90%|█████████ | 886/980 [01:00<00:06, 15.10it/s]\u001b[A\n",
      " 91%|█████████ | 888/980 [01:00<00:06, 15.02it/s]\u001b[A\n",
      " 91%|█████████ | 890/980 [01:00<00:05, 15.07it/s]\u001b[A\n",
      " 91%|█████████ | 892/980 [01:00<00:05, 15.02it/s]\u001b[A\n",
      " 91%|█████████ | 894/980 [01:00<00:05, 15.04it/s]\u001b[A\n",
      " 91%|█████████▏| 896/980 [01:00<00:05, 15.09it/s]\u001b[A\n",
      " 92%|█████████▏| 898/980 [01:00<00:05, 14.96it/s]\u001b[A\n",
      " 92%|█████████▏| 900/980 [01:00<00:05, 15.10it/s]\u001b[A\n",
      " 92%|█████████▏| 902/980 [01:01<00:05, 15.17it/s]\u001b[A\n",
      " 92%|█████████▏| 904/980 [01:01<00:05, 15.17it/s]\u001b[A\n",
      " 92%|█████████▏| 906/980 [01:01<00:04, 15.04it/s]\u001b[A\n",
      " 93%|█████████▎| 908/980 [01:01<00:04, 15.08it/s]\u001b[A\n",
      " 93%|█████████▎| 910/980 [01:01<00:04, 15.10it/s]\u001b[A\n",
      " 93%|█████████▎| 912/980 [01:01<00:04, 14.86it/s]\u001b[A\n",
      " 93%|█████████▎| 914/980 [01:01<00:04, 14.94it/s]\u001b[A\n",
      " 93%|█████████▎| 916/980 [01:02<00:04, 14.98it/s]\u001b[A\n",
      " 94%|█████████▎| 918/980 [01:02<00:04, 13.58it/s]\u001b[A\n",
      " 94%|█████████▍| 920/980 [01:02<00:04, 13.20it/s]\u001b[A\n",
      " 94%|█████████▍| 922/980 [01:02<00:04, 13.13it/s]\u001b[A\n",
      " 94%|█████████▍| 924/980 [01:02<00:04, 13.16it/s]\u001b[A\n",
      " 94%|█████████▍| 926/980 [01:02<00:03, 13.54it/s]\u001b[A\n",
      " 95%|█████████▍| 928/980 [01:02<00:03, 14.04it/s]\u001b[A\n",
      " 95%|█████████▍| 930/980 [01:03<00:03, 14.18it/s]\u001b[A\n",
      " 95%|█████████▌| 932/980 [01:03<00:03, 14.44it/s]\u001b[A\n",
      " 95%|█████████▌| 934/980 [01:03<00:03, 14.55it/s]\u001b[A\n",
      " 96%|█████████▌| 936/980 [01:03<00:02, 14.84it/s]\u001b[A\n",
      " 96%|█████████▌| 938/980 [01:03<00:02, 14.86it/s]\u001b[A\n",
      " 96%|█████████▌| 940/980 [01:03<00:02, 14.84it/s]\u001b[A\n",
      " 96%|█████████▌| 942/980 [01:03<00:02, 14.83it/s]\u001b[A\n",
      " 96%|█████████▋| 944/980 [01:04<00:02, 14.79it/s]\u001b[A\n",
      " 97%|█████████▋| 946/980 [01:04<00:02, 14.88it/s]\u001b[A\n",
      " 97%|█████████▋| 948/980 [01:04<00:02, 14.82it/s]\u001b[A\n",
      " 97%|█████████▋| 950/980 [01:04<00:02, 14.90it/s]\u001b[A\n",
      " 97%|█████████▋| 952/980 [01:04<00:01, 14.90it/s]\u001b[A\n",
      " 97%|█████████▋| 954/980 [01:04<00:01, 15.01it/s]\u001b[A\n",
      " 98%|█████████▊| 956/980 [01:04<00:01, 15.04it/s]\u001b[A\n",
      " 98%|█████████▊| 958/980 [01:04<00:01, 14.93it/s]\u001b[A\n",
      " 98%|█████████▊| 960/980 [01:05<00:01, 15.01it/s]\u001b[A\n",
      " 98%|█████████▊| 962/980 [01:05<00:01, 14.79it/s]\u001b[A\n",
      " 98%|█████████▊| 964/980 [01:05<00:01, 14.78it/s]\u001b[A\n",
      " 99%|█████████▊| 966/980 [01:05<00:00, 14.89it/s]\u001b[A\n",
      " 99%|█████████▉| 968/980 [01:05<00:00, 15.01it/s]\u001b[A\n",
      " 99%|█████████▉| 970/980 [01:05<00:00, 14.95it/s]\u001b[A\n",
      " 99%|█████████▉| 972/980 [01:05<00:00, 15.18it/s]\u001b[A\n",
      " 99%|█████████▉| 974/980 [01:06<00:00, 14.98it/s]\u001b[A\n",
      "100%|█████████▉| 976/980 [01:06<00:00, 15.09it/s]\u001b[A\n",
      "100%|██████████| 980/980 [01:06<00:00, 14.77it/s]\u001b[A\n",
      "516it [02:34,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:   6000,  Train Loss: 0.088,  Train Acc: 95.31%,  Val Loss:  0.18,  Val Acc: 94.34%,  Time: 1433.8142 \n",
      "No optimization for a long time, auto-stopping...\n"
     ]
    }
   ],
   "source": [
    "#config = config()\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "\n",
    "print('Loading dataset')\n",
    "train_dataset = BertDataset(config.train_path)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=config.batch_size,\n",
    "                                  collate_fn=collate_fn,\n",
    "                                  shuffle=True)\n",
    "print('Loading dataset1')\n",
    "dev_dataset = BertDataset(config.valid_path)\n",
    "dev_dataloader = DataLoader(dev_dataset,\n",
    "                                batch_size=config.batch_size,\n",
    "                                collate_fn=collate_fn,\n",
    "                                shuffle=True)\n",
    "'''print('Loading dataset2')\n",
    "test_dataset = BertDataset(config.test_path, tokenizer=tokenizer, word=True)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 collate_fn=collate_fn)'''\n",
    "print('load network')\n",
    "#model = Model(config).to(config.device)\n",
    "    # 初始化参数\n",
    "\n",
    "print('training model')\n",
    "train(config, model, train_dataloader, dev_dataloader)\n",
    "    # test(config, model, test_dataloader)  # 只测试模型的效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T07:11:32.342919Z",
     "iopub.status.busy": "2023-02-06T07:11:32.342517Z",
     "iopub.status.idle": "2023-02-06T07:12:38.496711Z",
     "shell.execute_reply": "2023-02-06T07:12:38.495546Z",
     "shell.execute_reply.started": "2023-02-06T07:11:32.342887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 980/980 [01:02<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.18,  Test Acc: 94.36%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          体育     0.9931    0.9775    0.9852      9870\n",
      "          彩票     0.9608    0.9473    0.9540       569\n",
      "          财经     0.9310    0.8537    0.8907      2782\n",
      "          游戏     0.9314    0.8758    0.9027      1828\n",
      "          家居     0.9408    0.9191    0.9298      2436\n",
      "          房产     0.9535    0.8451    0.8960      1504\n",
      "          社会     0.9404    0.9434    0.9419      3814\n",
      "          星座     0.9752    0.7351    0.8383       268\n",
      "          娱乐     0.9368    0.9774    0.9567      6945\n",
      "          时政     0.9221    0.9155    0.9188      4732\n",
      "          时尚     0.9519    0.9083    0.9296      1003\n",
      "          科技     0.9448    0.9412    0.9430     12211\n",
      "          股票     0.9193    0.9601    0.9393     11573\n",
      "          教育     0.9386    0.9672    0.9527      3143\n",
      "\n",
      "    accuracy                         0.9436     62678\n",
      "   macro avg     0.9457    0.9119    0.9270     62678\n",
      "weighted avg     0.9440    0.9436    0.9434     62678\n",
      "\n",
      "Confusion Matrix...\n",
      "[[ 9648    17     1     1     4     2     7     0   146    13     4     6\n",
      "     13     8]\n",
      " [   12   539     0     0     1     1    12     0     3     0     0     1\n",
      "      0     0]\n",
      " [    0     1  2375     0     5     4    10     0    13    20     1    27\n",
      "    320     6]\n",
      " [    1     0     0  1601     7     5     3     0    28     4     6   161\n",
      "      8     4]\n",
      " [    3     0     4     3  2239    17     3     4    47     8    12    55\n",
      "     36     5]\n",
      " [    1     0     9     1    52  1271    14     0    26    21     2    11\n",
      "     92     4]\n",
      " [    6     2     5     0     1     3  3598     0    40    70     1    57\n",
      "      5    26]\n",
      " [    1     0     0     0     2     0     0   197     2     0     1     2\n",
      "      0    63]\n",
      " [   28     0     0     8    10     2    15     0  6788    25    13    26\n",
      "     22     8]\n",
      " [    7     2     6     1     1     8    41     0    15  4332     3   110\n",
      "    171    35]\n",
      " [    0     0     0     1    19     0     4     1    45     9   911     5\n",
      "      1     7]\n",
      " [    6     0    14   101    28     2    75     0    64    95     3 11493\n",
      "    301    29]\n",
      " [    1     0   136     2     9    18     9     0    15    76     0   192\n",
      "  11111     4]\n",
      " [    1     0     1     0     2     0    35     0    14    25     0    19\n",
      "      6  3040]]\n",
      "Time usage: 62.2958\n"
     ]
    }
   ],
   "source": [
    "print('Loading dataset2')\n",
    "test_dataset = BertDataset(config.test_path)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 collate_fn=collate_fn)\n",
    "test(config, model, test_dataloader)  # 只测试模型的效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
